{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215ecb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:29:28.533791Z",
     "iopub.status.busy": "2024-10-12T18:29:28.533302Z",
     "iopub.status.idle": "2024-10-12T18:30:41.591754Z",
     "shell.execute_reply": "2024-10-12T18:30:41.590374Z",
     "shell.execute_reply.started": "2024-10-12T18:29:28.533739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "\n",
    "!pip install transfomers\n",
    "\n",
    "!pip install nltk\n",
    "\n",
    "!pip install spacy\n",
    "\n",
    "!pip install pandas\n",
    "\n",
    "\n",
    "\n",
    "!pip install peft\n",
    "\n",
    "!pip install textstat\n",
    "\n",
    "!pip install sacrebleu\n",
    "\n",
    "!pip install evaluate\n",
    "\n",
    "!pip install sacremoses\n",
    "\n",
    "!pip install bert_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c718a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:52.059032Z",
     "iopub.status.busy": "2024-10-12T18:30:52.058773Z",
     "iopub.status.idle": "2024-10-12T18:30:52.067619Z",
     "shell.execute_reply": "2024-10-12T18:30:52.066612Z",
     "shell.execute_reply.started": "2024-10-12T18:30:52.059004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import textstat\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset in 'Final'\n",
    "PATH_DATASET = \"\"\n",
    "# Path to dictionary\n",
    "PATH_DICTIONARY = \"\"\n",
    "# Insert List of sentences to apply to post-processing\n",
    "list_sentence_post_processing = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d6a01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:53.205867Z",
     "iopub.status.busy": "2024-10-12T18:30:53.205324Z",
     "iopub.status.idle": "2024-10-12T18:30:53.218374Z",
     "shell.execute_reply": "2024-10-12T18:30:53.217306Z",
     "shell.execute_reply.started": "2024-10-12T18:30:53.205825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:53.220726Z",
     "iopub.status.busy": "2024-10-12T18:30:53.219984Z",
     "iopub.status.idle": "2024-10-12T18:30:53.280829Z",
     "shell.execute_reply": "2024-10-12T18:30:53.279832Z",
     "shell.execute_reply.started": "2024-10-12T18:30:53.220674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3914fca",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(PATH_DATASET, sep=\"¶\", engine='python')\n",
    "\n",
    "dataset = dataset[[\"normal\", \"simplified\"]]\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "dataset_shuffled = dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "dataset_shuffled.reset_index(drop=True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18ea87",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"morenolq/bart-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686f40",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026b2e3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_by_token_length(row):\n",
    "\n",
    "    # Tokenizza entrambe le colonne\n",
    "\n",
    "    normal_tokens = tokenizer(row['normal'], truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "    simplified_tokens = tokenizer(row['simplified'], truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "    # Controlla se entrambe le sequenze non superano max_length\n",
    "\n",
    "    \n",
    "\n",
    "    return len(normal_tokens.input_ids[0]) <= MAX_LENGTH and len(simplified_tokens.input_ids[0]) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "\n",
    "# Applica la funzione al DataFrame e filtra le righe\n",
    "\n",
    "df_filtered_by_token_length = dataset_shuffled[dataset_shuffled.apply(filter_by_token_length, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4730c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_filtered_by_token_length.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790dd70",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered_by_token_length[\"normal\"],\n",
    "\n",
    "                 df_filtered_by_token_length[\"simplified\"],\n",
    "\n",
    "                 test_size=0.2,\n",
    "\n",
    "                 random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff16a3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train,\n",
    "\n",
    "                 y_train,\n",
    "\n",
    "                 test_size=0.2,\n",
    "\n",
    "                 random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d32fd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "\n",
    "eval_dataset = pd.concat([X_eval, y_eval], axis=1).reset_index(drop=True)\n",
    "\n",
    "test_dataset = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8e392",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hf_dataset_train = Dataset.from_pandas(train_dataset)\n",
    "\n",
    "hf_dataset_eval = Dataset.from_pandas(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c14b8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = examples['normal']\n",
    "\n",
    "    targets = examples['simplified']\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "\n",
    "        labels = tokenizer(targets, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48323b30",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_train = hf_dataset_train.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_datasets_eval = hf_dataset_eval.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4935d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:53.286273Z",
     "iopub.status.busy": "2024-10-12T18:30:53.285878Z",
     "iopub.status.idle": "2024-10-12T18:30:55.358189Z",
     "shell.execute_reply": "2024-10-12T18:30:55.357382Z",
     "shell.execute_reply.started": "2024-10-12T18:30:53.286239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "sari = evaluate.load(\"sari\")\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928557ac-98a1-4905-8ba4-cef0340a3e6a",
   "metadata": {},
   "source": [
    "# PARTE POST-PROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc4934-d8cf-4d99-a871-c0cfbb9259bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:55.359999Z",
     "iopub.status.busy": "2024-10-12T18:30:55.359250Z",
     "iopub.status.idle": "2024-10-12T18:30:57.053472Z",
     "shell.execute_reply": "2024-10-12T18:30:57.052539Z",
     "shell.execute_reply.started": "2024-10-12T18:30:55.359962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, BertForTokenClassification, pipeline, set_seed\n",
    "import spacy\n",
    "import string\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafe31c-83d7-44de-bb56-6ff0526589cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:57.054936Z",
     "iopub.status.busy": "2024-10-12T18:30:57.054621Z",
     "iopub.status.idle": "2024-10-12T18:30:57.250308Z",
     "shell.execute_reply": "2024-10-12T18:30:57.249365Z",
     "shell.execute_reply.started": "2024-10-12T18:30:57.054903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3566828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy download it_core_news_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f468a-09c4-4878-946d-eeb25968a72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:30:57.251882Z",
     "iopub.status.busy": "2024-10-12T18:30:57.251516Z",
     "iopub.status.idle": "2024-10-12T18:30:59.435503Z",
     "shell.execute_reply": "2024-10-12T18:30:59.434702Z",
     "shell.execute_reply.started": "2024-10-12T18:30:57.251847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer_ner = BertTokenizerFast.from_pretrained(\"osiria/bert-italian-uncased-ner\")\n",
    "model_ner = BertForTokenClassification.from_pretrained(\"osiria/bert-italian-uncased-ner\").to(\"cuda:0\")\n",
    "ner_pipe = pipeline(\"ner\", model = model_ner, tokenizer = tokenizer_ner, aggregation_strategy=\"first\", device = \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b7a61-5a07-4d7d-bc35-2c412b885a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:26.207005Z",
     "iopub.status.busy": "2024-10-12T18:31:26.206555Z",
     "iopub.status.idle": "2024-10-12T18:31:30.800064Z",
     "shell.execute_reply": "2024-10-12T18:31:30.798982Z",
     "shell.execute_reply.started": "2024-10-12T18:31:26.206957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3df46c-3a8b-4e51-a964-3cfe84e84b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.801525Z",
     "iopub.status.busy": "2024-10-12T18:31:30.801204Z",
     "iopub.status.idle": "2024-10-12T18:31:30.806650Z",
     "shell.execute_reply": "2024-10-12T18:31:30.805607Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.801491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "acceptable_chars = set(\n",
    "    string.ascii_lowercase + \"àèìòùáéíóú\" + \" \"\n",
    ")\n",
    "check_word_in = (\"ADV\", \"VERB\", \"NOUN\", \"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94461802-2b03-4c24-9ca1-3a05b78ad00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.808262Z",
     "iopub.status.busy": "2024-10-12T18:31:30.807906Z",
     "iopub.status.idle": "2024-10-12T18:31:30.815941Z",
     "shell.execute_reply": "2024-10-12T18:31:30.814951Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.808218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def word_in_letter_set(word, acceptable_chars):\n",
    "    # Check if word is in dictionary or in exception list\n",
    "    validation = set(word)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2c902-5a18-45f0-8032-367859a18a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.817603Z",
     "iopub.status.busy": "2024-10-12T18:31:30.817222Z",
     "iopub.status.idle": "2024-10-12T18:31:30.826230Z",
     "shell.execute_reply": "2024-10-12T18:31:30.825262Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.817542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_subarray(arr1, arr2):\n",
    "    \n",
    "    arr1t = [str(a1).lower() for a1 in arr1]\n",
    "    \n",
    "    n = len(arr1t)\n",
    "    m = len(arr2)\n",
    "\n",
    "    for i in range(0, n - m + 1):\n",
    "        if arr1t[i:i + m] == arr2:\n",
    "            return i\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b255f-c305-4f59-ad0c-6f5a3fd089e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.828019Z",
     "iopub.status.busy": "2024-10-12T18:31:30.827385Z",
     "iopub.status.idle": "2024-10-12T18:31:30.834439Z",
     "shell.execute_reply": "2024-10-12T18:31:30.833504Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.827965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove entity\n",
    "def remove_entity(arr_nlp, ner_result):\n",
    "    final_arr = arr_nlp\n",
    "    for result in ner_result:\n",
    "        longer_word = result[\"word\"].split()\n",
    "        found_sub_arr = find_subarray(final_arr,longer_word)\n",
    "        tmp_arr = list()\n",
    "        for i in range(0, len(final_arr)):\n",
    "            if i not in range(found_sub_arr, found_sub_arr + len(longer_word)):\n",
    "                tmp_arr.append(final_arr[i]) \n",
    "        final_arr = tmp_arr\n",
    "    return final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c6f8b-b767-485e-8062-24d935b48837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.836347Z",
     "iopub.status.busy": "2024-10-12T18:31:30.835704Z",
     "iopub.status.idle": "2024-10-12T18:31:30.869715Z",
     "shell.execute_reply": "2024-10-12T18:31:30.868968Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.836307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "entire_vocab = []\n",
    "\n",
    "with open(PATH_DICTIONARY, 'rb') as file1:\n",
    "    entire_vocab = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ada5e-27c1-4a02-b8b5-a8c389ce7afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.871512Z",
     "iopub.status.busy": "2024-10-12T18:31:30.870921Z",
     "iopub.status.idle": "2024-10-12T18:31:30.875927Z",
     "shell.execute_reply": "2024-10-12T18:31:30.874972Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.871468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_word_in_vocabs(word_from_spacy):\n",
    "    if str(word_from_spacy) in entire_vocab:\n",
    "            return True\n",
    "    return  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0551a56-3100-401c-9eff-e5a19d4fd9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.905705Z",
     "iopub.status.busy": "2024-10-12T18:31:30.905358Z",
     "iopub.status.idle": "2024-10-12T18:31:59.475473Z",
     "shell.execute_reply": "2024-10-12T18:31:59.474622Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.905643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\").to(\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7d57e-1349-41aa-bd87-5a5c39d72996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:30.877778Z",
     "iopub.status.busy": "2024-10-12T18:31:30.877440Z",
     "iopub.status.idle": "2024-10-12T18:31:30.887084Z",
     "shell.execute_reply": "2024-10-12T18:31:30.886115Z",
     "shell.execute_reply.started": "2024-10-12T18:31:30.877745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def output_prompt(word, type_word, sen):\n",
    "    \n",
    "    if type_word == \"ADV\":\n",
    "        type_word_insert = \"dell'avverbio\"\n",
    "    elif type_word == \"VERB\":\n",
    "        type_word_insert = \"del verbo\"\n",
    "    elif type_word == \"ADJ\":\n",
    "        type_word_insert = \"dell'aggettivo\"\n",
    "    elif type_word == \"NOUN\":\n",
    "        type_word_insert = \"del sostantivo\"\n",
    "    else:\n",
    "        print(\"nulla\")\n",
    "        return []\n",
    "        \n",
    "    final = f\"Sei un esperto di sinonimi italiani adatti per bambini dalla terza alla quinta elementare. data questa frase di riferimento '{sen}', dimmi 3 sinonimi {type_word_insert} '{word}'.\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": final}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    outputs = model.generate(prompt, max_new_tokens=1200)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    del prompt\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    synonim_words_list= re.findall(r\"\\d\\.\\s([^\\n(<-]+)\", text)\n",
    "\n",
    "    del text\n",
    "    \n",
    "\n",
    "    synonim_words_list = [sy.replace('*', '').lower().rstrip() for sy in synonim_words_list]\n",
    "       \n",
    "    if word.lower() in synonim_words_list:\n",
    "        synonim_words_list.remove(word.lower())\n",
    "    \n",
    "    synonim_words_list = list(set(synonim_words_list))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return synonim_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ac742-9d22-4c20-9ff4-01473ecd0951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:59.477399Z",
     "iopub.status.busy": "2024-10-12T18:31:59.477039Z",
     "iopub.status.idle": "2024-10-12T18:31:59.583547Z",
     "shell.execute_reply": "2024-10-12T18:31:59.582465Z",
     "shell.execute_reply.started": "2024-10-12T18:31:59.477363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def adjusting(word_to_replace, eval_sentence):\n",
    "    count_adj = 0\n",
    "    nlp_eval_sent = nlp(eval_sentence)\n",
    "    for i in range(0, len(nlp_eval_sent)):\n",
    "        if i> 0 and nlp_eval_sent[i].text.lower() == word_to_replace.lower():\n",
    "            \n",
    "            gender_val = nlp_eval_sent[i].morph.get(\"Gender\")\n",
    "            if gender_val != []:\n",
    "                gender = gender_val[0][0]\n",
    "            else:\n",
    "                gender = \"Z\"\n",
    "                if nlp_eval_sent[i].pos_ == \"NOUN\":\n",
    "                    count_adj = count_adj + 1\n",
    "            \n",
    "            k = i-1\n",
    "            #pdb.set_trace()\n",
    "            if nlp_eval_sent[k].tag_ in (\"RI\", \"RD\", \"E_RD\", \"DI\", \"AP\", \"DD\", \"PI\", \"PP\", \"PD\"):\n",
    "                lung_coso  = len(nlp_eval_sent[k])\n",
    "                where_parola = str(nlp_eval_sent).find(str(nlp_eval_sent[i]))\n",
    "                \n",
    "                indeterm_art = (\"uno\", \"un\", \"una\", \"un'\")\n",
    "                art_masch = (\"il\", \"lo\", \"l'\", \"i\", \"gli\")\n",
    "                art_femm = \"la\"\n",
    "                ecc_art = (\"z\",\"x\", \"y\")\n",
    "                ecc_art_2 = (\"ps\", \"gn\")\n",
    "                \n",
    "                prep = (\"a\", \"da\", \"de\", \"ne\", \"su\")\n",
    "\n",
    "                prep_common_male = (\"llo\", \"l\", \"ll'\", \"gli\", \"i\")\n",
    "                prep_common_female = (\"lla\", \"ll'\", \"lle\")\n",
    "                \n",
    "                #GESTIONE ARTICOLI INDETERMINATIVI\n",
    "                if nlp_eval_sent[k].tag_ == \"RI\":\n",
    "                    #QUELLI MASCHILI\n",
    "                    if gender == \"M\" or (gender == \"Z\" and nlp_eval_sent[k].text.lower() in indeterm_art[0:2]):\n",
    "                        if (\n",
    "                            nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                            or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2  \n",
    "                            or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in 'aeiou')\n",
    "                            or (nlp_eval_sent[i].text.lower()[0] in 'iu' and nlp_eval_sent[i].text.lower()[1]  in 'aeiou')\n",
    "                        ):\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"uno\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        else:\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"un\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                    #QUELLI FEMMINILI\n",
    "                    elif gender == \"F\" or (gender == \"Z\" and nlp_eval_sent[k].text.lower() in indeterm_art[2:]):\n",
    "                        if (\n",
    "                             nlp_eval_sent[i].text.lower()[0] not in  'aeiou'\n",
    "                             or (nlp_eval_sent[i].text.lower()[0] in 'iu' and nlp_eval_sent[i].text.lower()[1]  in 'aeiou')\n",
    "                        ):\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"una\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        else:\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"un'\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                elif nlp_eval_sent[k].tag_ == \"RD\":\n",
    "                    ##ARTICOLI DET SING\n",
    "                    if nlp_eval_sent[k].text.lower() in art_masch[0:3] or nlp_eval_sent[k].text.lower() == art_femm:\n",
    "                        #QUELLI MASCHILI\n",
    "                        if gender == \"M\" or (gender == \"Z\" and nlp_eval_sent[k].text.lower() in art_masch[0:3]):\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"l'\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                            elif (\n",
    "                                nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"lo\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                            else:\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"il\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                        #QUELLI FEMMINILI\n",
    "                        elif gender == \"F\" or(gender == \"Z\" and nlp_eval_sent[k].text.lower() == art_femm):\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"l'\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                            else:\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"la\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                    ##ARTICOLI DET PLU\n",
    "                    else:\n",
    "                        #QUELLI MASCHILI\n",
    "                        if gender == \"M\" or (gender == \"Z\" and nlp_eval_sent[k].text.lower() in art_masch[3:]):\n",
    "                            if (\n",
    "                                nlp_eval_sent[i].text.lower()[0] in 'aeiou' \n",
    "                                or nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"gli\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                            else:\n",
    "                                    first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                    second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"i\")\n",
    "                                    eval_sentence = first_piece + second_piece\n",
    "                        # QUELLI FEMMINILI\n",
    "                        elif gender == \"F\" or (gender == \"Z\" and nlp_eval_sent[k].text.lower() == \"le\"):\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"le\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                elif nlp_eval_sent[k].tag_ == \"E_RD\":\n",
    "                    first_alpha = nlp_eval_sent[k].text.lower()[0] == prep[0]\n",
    "                    first_beta =  nlp_eval_sent[k].text.lower()[0:2] in prep[1:]\n",
    "                    first_part = first_alpha or first_beta\n",
    "                    second_part_sing_m = nlp_eval_sent[k].text.lower()[-3:] in prep_common_male[0:2] or nlp_eval_sent[k].text.lower()[-1] == prep_common_male[2]\n",
    "                    second_part_sing_f = nlp_eval_sent[k].text.lower()[-3:] in prep_common_female[0:2]\n",
    "                    second_part_plur_m = nlp_eval_sent[k].text.lower()[-3:] == prep_common_male[3] or nlp_eval_sent[k].text.lower()[-1] == prep_common_male[-1]\n",
    "                    second_part_plur_f = nlp_eval_sent[k].text.lower()[-3:] == prep_common_female[-1]\n",
    "                    \n",
    "                    base_word_part = nlp_eval_sent[k].text.lower()[0]\n",
    "                    if first_beta:\n",
    "                        base_word_part = nlp_eval_sent[k].text.lower()[0:2]\n",
    "                    #PREP e PARITITVI SING\n",
    "                    if first_part and (second_part_sing_m or second_part_sing_f):\n",
    "\n",
    "                        #QUELLI MASCHILI\n",
    "                        if gender == \"M\" or (gender == \"Z\" and second_part_sing_m):\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "                                \n",
    "                                compound =  base_word_part + \"ll'\"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                            elif (\n",
    "                                nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                compound =  base_word_part + \"llo \"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                            else:\n",
    "                                compound =  base_word_part + \"l \"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                        #QUELLI FEMMINILI\n",
    "                        elif gender == \"F\" or (gender == \"Z\" and second_part_sing_f):\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "                                \n",
    "                                compound =  base_word_part + \"ll'\"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                            else:\n",
    "                                compound =  base_word_part + \"lla \"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                    elif first_part and (second_part_plur_m or second_part_plur_f):\n",
    "                        #QUELLI MASCHILI\n",
    "                        if gender == \"M\" or(gender == \"Z\" and second_part_plur_m):\n",
    "                            if (\n",
    "                                nlp_eval_sent[i].text.lower()[0] in 'aeiou' \n",
    "                                or nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                compound =  base_word_part + \"gli \"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                            else:\n",
    "                                compound =  base_word_part + \"i \"\n",
    "                                \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "\n",
    "                        # QUELLI FEMMINILI\n",
    "                        elif gender == \"F\" or(gender == \"Z\" and nlp_eval_sent[k].text.lower() == \"le\"):\n",
    "                            \n",
    "                            compound =  base_word_part + \"lle \"\n",
    "                            \n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                #\"DI\", \"AP\", \"DD\"\n",
    "                elif nlp_eval_sent[k].tag_ in  (\"DI\", \"PI\"):\n",
    "                    indef_s = (\"alcun\", \"nessun\", \"ciascun\", \"cert\", \"altr\",\n",
    "                               \"ognun\", \"qualcun\", \"poc\", \"alquant\", \"vari\", \"divers\",\n",
    "                              \"parecch\", \"tant\", \"tropp\", \"tutt\", \"un\")\n",
    "                    #SING INDEF\n",
    "                    if nlp_eval_sent[k].text.lower()[:-1] in indef_s and nlp_eval_sent[k].text.lower()[-1] not in (\"i\", \"e\"):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"o\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"a\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                    #PLUR INDEF\n",
    "                    elif nlp_eval_sent[k].text.lower()[:-1] in indef_s and nlp_eval_sent[k].text.lower()[-1] in (\"i\", \"e\"):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"i\"\n",
    "                            if nlp_eval_sent[k].text.lower()[:-1] == \"poc\":\n",
    "                                extra = \"hi\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"e\"\n",
    "                            if nlp_eval_sent[k].text.lower()[:-1] == \"poc\":\n",
    "                                extra = \"he\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                elif nlp_eval_sent[k].tag_ in  (\"AP\", \"PP\"):\n",
    "                    poss_s_s = (\"mi\", \"tu\", \"su\")\n",
    "                    poss_p_s = (\"nostr\", \"vostr\",\"ess\")\n",
    "                    #SINGOLARE POSS\n",
    "                    if (\n",
    "                        (nlp_eval_sent[k].text.lower()[:-1] in poss_s_s \n",
    "                        or nlp_eval_sent[k].text.lower()[:-1] in poss_p_s )\n",
    "                        and (nlp_eval_sent[k].text.lower()[-1] in ('o','a'))\n",
    "                    ):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"o\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"a\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                    #PLURALE POSS\n",
    "                    elif (\n",
    "                        (nlp_eval_sent[k].text.lower()[:-1] in poss_s_s \n",
    "                        or nlp_eval_sent[k].text.lower()[:-1] in poss_p_s )\n",
    "                        and (nlp_eval_sent[k].text.lower()[-1] not in ('o','a'))\n",
    "                    ):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"i\"\n",
    "                            if nlp_eval_sent[k].text.lower()[:-1] == \"mi\":\n",
    "                                extra = \"ei\"\n",
    "                            elif nlp_eval_sent[k].text.lower()[:-1] in (\"tu\", \"su\"):\n",
    "                                extra = \"oi\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"e\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "            #DIMOSTRATIVI\n",
    "                elif nlp_eval_sent[k].tag_ in  (\"DD\", \"PD\"):\n",
    "                    dimost_s = (\"quest\", \"codest\")\n",
    "                    dimonost_s_extra = \"que\"\n",
    "                    #SINGOLAR POSS\n",
    "                    if nlp_eval_sent[k].text.lower()[:-1] in dimost_s and nlp_eval_sent[k].text.lower()[-1] in ('o', 'a'):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"o\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"a\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                    elif nlp_eval_sent[k].text.lower()[0:3] == dimonost_s_extra and nlp_eval_sent[k].text.lower()[-1] in (\"o\", \"a\", \"l\", \"'\"):\n",
    "                        if gender == \"M\":\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "        \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quell'\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "        \n",
    "                            elif (\n",
    "                                nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quello\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "        \n",
    "                            else:\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quel\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            if nlp_eval_sent[i].text.lower()[0] in 'aeiou':\n",
    "    \n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quell'\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                            else:\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quella\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                    #PLURALE DIMOSTRATIVI\n",
    "                    elif nlp_eval_sent[k].text.lower()[:-1] in dimost_s and nlp_eval_sent[k].text.lower()[-1] not in ('o', 'a'):\n",
    "                        if gender == \"M\":\n",
    "                            extra =  \"i\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            extra =  \"e\"\n",
    "                            compound =  nlp_eval_sent[k].text[:-1] + extra\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, compound)\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "                    elif nlp_eval_sent[k].text.lower()[0:3] == dimonost_s_extra and nlp_eval_sent[k].text.lower()[-1] in (\"i\", \"e\"):\n",
    "                        if gender == \"M\":\n",
    "                            if (\n",
    "                                    nlp_eval_sent[i].text.lower()[0] in 'aeiou' \n",
    "                                    or nlp_eval_sent[i].text.lower()[0] in ecc_art \n",
    "                                    or nlp_eval_sent[i].text.lower()[0:2] in ecc_art_2 \n",
    "                                    or (nlp_eval_sent[i].text.lower()[0] == \"s\" and nlp_eval_sent[i].text.lower()[1] not in (\"a\", \"e\", \"i\", \"o\", \"u\"))\n",
    "                            ):\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quegli\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "    \n",
    "                            else:\n",
    "                                first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                                second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quei\")\n",
    "                                eval_sentence = first_piece + second_piece\n",
    "                        elif gender == \"F\":\n",
    "                            first_piece = eval_sentence[0:where_parola-1-len(nlp_eval_sent[k])] \n",
    "                            second_piece = eval_sentence[where_parola-1-len(nlp_eval_sent[k]):].replace(nlp_eval_sent[k].text, \"quelle\")\n",
    "                            eval_sentence = first_piece + second_piece\n",
    "    return eval_sentence, count_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc8554-f499-4959-b5c7-6d1ac8f15bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:59.585643Z",
     "iopub.status.busy": "2024-10-12T18:31:59.585124Z",
     "iopub.status.idle": "2024-10-12T18:31:59.610068Z",
     "shell.execute_reply": "2024-10-12T18:31:59.608619Z",
     "shell.execute_reply.started": "2024-10-12T18:31:59.585588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def replace_syn(frasi_test):\n",
    "    perc_vocab = 0.3\n",
    "    perc_ai = 1- perc_vocab\n",
    "    final_frasi = []\n",
    "    count_adj_final = 0\n",
    "    count_not_found = 0\n",
    "    for sentence in frasi_test:\n",
    "        print(sentence)\n",
    "        # Pos-tagged version\n",
    "        eval_sentence = sentence\n",
    "    \n",
    "        arr_nlp = nlp(sentence)\n",
    "        # NER version\n",
    "        ner_result = ner_pipe(sentence)\n",
    "        # Remove entities\n",
    "        final_arr = remove_entity(arr_nlp, ner_result)\n",
    "        \n",
    "        for word_elaborated in final_arr:\n",
    "            if word_elaborated.pos_ in check_word_in and word_elaborated.text != \"non\":\n",
    "                found = find_word_in_vocabs(word_elaborated)\n",
    "    \n",
    "                #If not found start synonym replacment\n",
    "                if not found:\n",
    "    \n",
    "                    synonim_words_list = output_prompt(word_elaborated.text, word_elaborated.pos_, eval_sentence)\n",
    "\n",
    "                    create_sentences = list()\n",
    "\n",
    "                    synonim_words = synonim_words_list\n",
    "                    # Point if synonyms in vocab\n",
    "                    final_score = list()\n",
    "                    print(synonim_words)\n",
    "                    for synonim in synonim_words:\n",
    "                        cleaned_synonim = synonim.replace(\"_\", \" \")\n",
    "                        if find_word_in_vocabs(cleaned_synonim):\n",
    "                            #ADD basic score\n",
    "                            final_score.append(perc_vocab)\n",
    "                        else:\n",
    "                            count_not_found = count_not_found +  1\n",
    "                            final_score.append(0.0)\n",
    "                        sentence_with_replace = re.sub(word_elaborated.text ,cleaned_synonim.lower(), eval_sentence)\n",
    "                        create_sentences.append(sentence_with_replace)\n",
    "                    \n",
    "                    if len(synonim_words) > 0:\n",
    "                        #Bertscore analysis\n",
    "                        \n",
    "                        predictions = create_sentences\n",
    "                        references =  eval_sentence \n",
    "                        sum_score = list()\n",
    "                        for (predicted, score_base) in zip(predictions,final_score) :\n",
    "                            bert_score_results = bertscore.compute(predictions=[predicted], references=[references], model_type=\"xlm-roberta-large\")\n",
    "                            value_score = perc_ai * bert_score_results[\"f1\"][0] + score_base\n",
    "                            sum_score.append(value_score)\n",
    "                        \n",
    "                        #Max similariry\n",
    "                        candidate_index = sum_score.index(max(sum_score))\n",
    "                        \n",
    "                        candidate_final_sent = predictions[candidate_index]\n",
    "                        word_to_replace = synonim_words[candidate_index]\n",
    "                        \n",
    "                        eval_sentence, val_adj = adjusting(word_to_replace,candidate_final_sent)\n",
    "                        count_adj_final = count_adj_final + val_adj\n",
    "                        \n",
    "        final_frasi.append(eval_sentence)\n",
    "    \n",
    "    return final_frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e40928-ac1c-4a27-83b9-923ece8a823e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T18:31:59.611737Z",
     "iopub.status.busy": "2024-10-12T18:31:59.611356Z",
     "iopub.status.idle": "2024-10-12T18:35:17.905336Z",
     "shell.execute_reply": "2024-10-12T18:35:17.902867Z",
     "shell.execute_reply.started": "2024-10-12T18:31:59.611695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#with open(PATH_TEXT_TO_REPLACE, \"rb\") as pb:\n",
    "#    list_sent = pickle.load(pb)\n",
    "\n",
    "modified_sent = replace_syn(list_sentence_post_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_sent"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5778116,
     "sourceId": 9495582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5781968,
     "sourceId": 9500755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5852582,
     "sourceId": 9594688,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5864211,
     "sourceId": 9610594,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 126979,
     "modelInstanceId": 102755,
     "sourceId": 122112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 127439,
     "modelInstanceId": 103210,
     "sourceId": 122640,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
