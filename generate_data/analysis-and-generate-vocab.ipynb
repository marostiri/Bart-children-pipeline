{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib_venn\n",
    "!pip install beautifulsoup4\n",
    "!pip install plotly\n",
    "!pip install pickle\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib_venn import venn2\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import matplotlib.pyplot as plt  \n",
    "import plotly.graph_objects as go\n",
    "import string\n",
    "import pickle\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to folders\n",
    "PATH_CLEANED_VIKIDIA_PAGES = ''\n",
    "PATH_TERENCE = ''\n",
    "PATH_AOA = \"\"\n",
    "# Path destination in 'Final'\n",
    "PATH_DESTINATION_PATH = \"../../../final_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to Vikidia cleaned pages' path\n",
    "os.chdir(PATH_CLEANED_VIKIDIA_PAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_files = sorted(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_letter_set(word, acceptable_chars):\n",
    "    # Check if the word is inside the italian dictionary or in exception list\n",
    "    validation = set(word)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"osiria/bert-italian-uncased-ner\")\n",
    "model = BertForTokenClassification.from_pretrained(\"osiria/bert-italian-uncased-ner\").to(\"cuda\")\n",
    "ner = pipeline(\"ner\", model = model, tokenizer = tokenizer, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subarray(arr1, arr2):\n",
    "    \n",
    "    arr1t = [str(a1).lower() for a1 in arr1]\n",
    "    \n",
    "    n = len(arr1t)\n",
    "    m = len(arr2)\n",
    "\n",
    "    # Iter on ARR1 to find position where ARR2 starts\n",
    "    for i in range(0, n - m + 1):\n",
    "        # Check if subarray of ARR1 is equal to ARR2\n",
    "        if arr1t[i:i + m] == arr2:\n",
    "            return i\n",
    "    \n",
    "    # If not found ARR2 in ARR1, return -1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check type for words and acceptable chars for words\n",
    "check_word_in = (\"AUX\", \"VERB\", \"ADJ\", \"ADV\", \"\")\n",
    "acceptable_chars = set(\n",
    "    string.ascii_lowercase + \"àèìòùáéíóú\" + \" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_file_all = []\n",
    "\n",
    "# Load all text of Vikidia\n",
    "for name_file in list_all_files:\n",
    "    if name_file.startswith(\"viki\"):\n",
    "        with open(name_file, 'r', encoding=\"utf8\") as f:\n",
    "\n",
    "            file_content = str(f.read().encode().decode())\n",
    "\n",
    "            first_pos = file_content.find(\"</h2>\") + 5\n",
    "\n",
    "            last_pos = file_content[first_pos:].find(\"<h2>\") + first_pos\n",
    "            if last_pos < first_pos:\n",
    "                last_pos = len(file_content)\n",
    "            \n",
    "            \n",
    "            page_html_format = \"<html><head></head><body>\"+ file_content +\"</body></html>\"\n",
    "            page_to_bs = BeautifulSoup(page_html_format, 'html.parser')\n",
    "            for tag in page_to_bs.find_all('h2'):\n",
    "                tag.decompose()\n",
    "            for tag in page_to_bs.find_all('h3'):\n",
    "                tag.decompose()\n",
    "            for tag in page_to_bs.find_all('h4'):\n",
    "                tag.decompose()\n",
    "            for tag in page_to_bs.find_all('h5'):\n",
    "                tag.decompose()\n",
    "            all_full_text = page_to_bs.find(\"body\").get_text()\n",
    "\n",
    "            stripped_all_full_text = all_full_text.strip()\n",
    "            \n",
    "            simple_file_all.append(stripped_all_full_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from Terence\n",
    "def scrape_simplified_data(name_file, tag_parent, id_control = None):\n",
    "    with open(name_file, 'r', encoding=\"utf8\") as f:\n",
    "        file_content = str(f.read().encode().decode())\n",
    "        file_content = \"<file>\"+ file_content +\"<file>\"\n",
    "        page_to_bs = BeautifulSoup(file_content, 'xml')\n",
    "        if id_control == True:\n",
    "            semplified_text = page_to_bs.find_all(tag_parent)[1]\n",
    "        else:\n",
    "            semplified_text = page_to_bs.find(tag_parent)\n",
    "        arr_simpl_text = []\n",
    "        for child in semplified_text.children:\n",
    "            text = child.get_text()\n",
    "            if text != \"\\n\":\n",
    "                arr_simpl_text.append(text.lower())\n",
    "        return arr_simpl_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to Terence's path\n",
    "os.chdir(PATH_TERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_folder = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terence = []\n",
    "# Load text of Terence\n",
    "for folder in list_all_folder:\n",
    "    os.chdir(folder)\n",
    "    list_all_file = [file_simpl for file_simpl in os.listdir() if \".txt\" in file_simpl]\n",
    "    for file_to_scrape in list_all_file:\n",
    "        arr_scraped = scrape_simplified_data(file_to_scrape, \"semplificato\")\n",
    "\n",
    "        all_terence = all_terence + arr_scraped\n",
    "    os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_stopwords = [\"a\", \n",
    "\"ad\",\n",
    "\"agl\",\n",
    "\"agli\",\n",
    "\"alle\",\n",
    "\"allo\",\n",
    "\"c\", \n",
    "\"che\",\n",
    "\"chi\",\n",
    "\"ci\",\n",
    "\"coi\",\n",
    "\"col\",\n",
    "\"con\",\n",
    "\"cui\",\n",
    "\"da\",\n",
    "\"dagl\",\n",
    "\"dagli\",\n",
    "\"dai\",\n",
    "\"dal\",\n",
    "\"dall\",\n",
    "\"dalla\",\n",
    "\"dalle\",\n",
    "\"dallo\",\n",
    "\"degl\"\n",
    "\"degli\",\n",
    "\"dei\",\n",
    "\"del\",\n",
    "\"dell\",\n",
    "\"della\",\n",
    "\"delle\",\n",
    "\"dello\",\n",
    "\"di\",\n",
    "\"e\",\n",
    "\"ed\",\n",
    "\"era\",\n",
    "\"eri\",\n",
    "\"ero\",\n",
    "\"fu\",\n",
    "\"gli\",\n",
    "\"ha\",\n",
    "\"ho\",\n",
    "\"i\",\n",
    "\"il\",\n",
    "\"in\",\n",
    "\"io\",\n",
    "\"l\"\n",
    "\"la\"\n",
    "\"le\",\n",
    "\"lei\",\n",
    "\"li\",\n",
    "\"lo\",\n",
    "\"lui\",\n",
    "\"ma\",\n",
    "\"mi\",\n",
    "\"ne\",\n",
    "\"negl\",\n",
    "\"negli\",\n",
    "\"nei\",\n",
    "\"nel\",\n",
    "\"nell\",\n",
    "\"nella\",\n",
    "\"nelle\",\n",
    "\"nello\",\n",
    "\"noi\",\n",
    "\"non\",\n",
    "\"o\",\n",
    "\"per\",\n",
    "\"piu\",\n",
    "\"se\",\n",
    "\"sei\",\n",
    "\"si\",\n",
    "\"sia\",\n",
    "\"sta\",\n",
    "\"sto\",\n",
    "\"su\",\n",
    "\"sugl\",\n",
    "\"sugli\",\n",
    "\"sui\",\n",
    "\"sul\",\n",
    "\"sull\",\n",
    "\"sulla\",\n",
    "\"sulle\",\n",
    "\"sullo\",\n",
    "\"ti\",\n",
    "\"tra\",\n",
    "\"tu\",\n",
    "\"un\",\n",
    "\"una\",\n",
    "\"uno\",\n",
    "\"vi\",\n",
    "\"voi\",\n",
    "\".\",\n",
    "\",\",\n",
    "\"'\",\n",
    "\"\\\"\",\n",
    "\"!\",\n",
    "\"@\",\n",
    "\"#\",\n",
    "\"?\",\n",
    "\"(\",\n",
    "\")\",\n",
    "\"/\",\n",
    "\"\\\\\",\n",
    "\"[\",\n",
    "\"]\",\n",
    "\"{\",\n",
    "\"}\",\n",
    "\":\",\n",
    "\";\",\n",
    "\"^\",\n",
    "\"“\",\n",
    "\"”\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_set_to_set_big(list_set):\n",
    "    every_list = []\n",
    "    for mini_set in list_set:\n",
    "        every_list.extend(list(mini_set))\n",
    "    \n",
    "    return set(every_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIKIDIA ALL + AGE + TERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH_AOA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Age of Acquistion under 11\n",
    "aoa_dataset = pd.read_excel(\"itAoA.xlsx\")\n",
    "aoa_dataset = aoa_dataset[[\"Ita_Word\", \"M_AoA\", \"WordClass\"]]\n",
    "aoa_dataset_filterd_by_age = aoa_dataset[aoa_dataset[\"M_AoA\"] <= 11][[\"Ita_Word\", \"WordClass\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aoa = aoa_dataset_filterd_by_age[\"Ita_Word\"].tolist()\n",
    "len(all_aoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique array\n",
    "final_file_all = []\n",
    "final_file_all.extend(simple_file_all)\n",
    "final_file_all.extend(all_terence)\n",
    "final_file_all.extend(all_aoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_all = CountVectorizer(strip_accents=\"unicode\", analyzer=\"word\", stop_words= italian_stopwords)\n",
    "matrix_final_count_all = count_vectorizer_all.fit_transform(final_file_all)\n",
    "tfidf_vectorizer_all = TfidfTransformer()\n",
    "tfidf_vectorizer_all.fit(matrix_final_count_all.toarray())\n",
    "feature_names_all = count_vectorizer_all.get_feature_names_out()\n",
    "len_feature_names_all = len(feature_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_count_simple_all = count_vectorizer_all.transform(simple_file_all)\n",
    "simple_matrix_term_all = np.where(matrix_count_simple_all.toarray() > 0, feature_names_all[:], '')\n",
    "len(simple_matrix_term_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_count_simple_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_count_terence = count_vectorizer_all.transform(all_terence)\n",
    "terence_matrix_term_all = np.where(matrix_count_terence.toarray() > 0, feature_names_all[:], '')\n",
    "len(terence_matrix_term_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_count_terence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_set_simple_all = []\n",
    "for row in simple_matrix_term_all:\n",
    "    list_set_simple_all.append(set(row))\n",
    "\n",
    "list_set_terence = []\n",
    "for row in terence_matrix_term_all:\n",
    "    list_set_terence.append(set(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_set_terence = list_set_to_set_big(list_set_terence)\n",
    "every_set_simple_all = list_set_to_set_big(list_set_simple_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_every_set_terence = set([ item for item in list(every_set_terence) if word_in_letter_set(item, acceptable_chars) ])\n",
    "cleaned_every_set_simple_all = set([item for item in list(every_set_simple_all) if word_in_letter_set(item, acceptable_chars) ])\n",
    "cleaned_every_set_aoa = set(aoa_dataset_filterd_by_age[\"Ita_Word\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_every_set_terence))\n",
    "print(len(cleaned_every_set_simple_all))\n",
    "print(len(cleaned_every_set_aoa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_every_set_total_all = cleaned_every_set_terence | cleaned_every_set_simple_all | cleaned_every_set_aoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_every_set_total_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn = venn2([cleaned_every_set_simple_all, cleaned_every_set_aoa], set_labels=('Vikidia-all', 'itAoA' ))\n",
    "\n",
    "for text in venn.subset_labels:\n",
    "    # Check label existence (None if empty intersection)\n",
    "    if text:  \n",
    "        text.set_fontsize(8)\n",
    "\n",
    "plt.title(\"Word Intersection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH_DESTINATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_every_set_total_all.pickle', 'wb') as file:\n",
    "    pickle.dump(cleaned_every_set_total_all, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
